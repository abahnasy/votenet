{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory is /home/abahnasy/Desktop/votenet/notebooks/..\n",
      "Base Directory is /home/abahnasy/Desktop/votenet/notebooks\n",
      "Data Directory is /home/abahnasy/Desktop/votenet/notebooks/../waymo_open_dataset/dataset\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "BASE_DIR = os.getcwd()\n",
    "# BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "ROOT_DIR = os.path.join(BASE_DIR, '..')\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'waymo_open_dataset', 'dataset')\n",
    "print(\"Root directory is {}\".format(ROOT_DIR))\n",
    "print(\"Base Directory is {}\".format(BASE_DIR))\n",
    "print(\"Data Directory is {}\".format(DATA_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input arguments\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "sys.path.append(ROOT_DIR)\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'utils')) # import Box class\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'waymo_open_dataset')) # import Box class\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "import dataset_pb2 as open_dataset\n",
    "import frame_utils\n",
    "from box_util import Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['segment-10072140764565668044_4060_000_4080_000_with_camera_labels.tfrecord',\n",
       " 'segment-10094743350625019937_3420_000_3440_000_with_camera_labels.tfrecord',\n",
       " 'segment-10075870402459732738_1060_000_1080_000_with_camera_labels.tfrecord',\n",
       " 'segment-10023947602400723454_1120_000_1140_000_with_camera_labels.tfrecord',\n",
       " 'segment-10061305430875486848_1080_000_1100_000_with_camera_labels.tfrecord',\n",
       " 'segment-10096619443888687526_2820_000_2840_000_with_camera_labels.tfrecord',\n",
       " 'segment-10017090168044687777_6380_000_6400_000_with_camera_labels.tfrecord',\n",
       " 'segment-10082223140073588526_6140_000_6160_000_with_camera_labels.tfrecord',\n",
       " 'segment-1005081002024129653_5313_150_5333_150_with_camera_labels.tfrecord',\n",
       " 'segment-10072231702153043603_5725_000_5745_000_with_camera_labels.tfrecord',\n",
       " 'segment-10107710434105775874_760_000_780_000_with_camera_labels.tfrecord']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all the segments in the folder\n",
    "data_split_dir = os.path.join(DATA_DIR, 'training')\n",
    "if not os.path.exists(data_split_dir):\n",
    "    raise Exception(\"Data Path is not found\")\n",
    "segments_list = os.listdir(data_split_dir)\n",
    "segments_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_folders_for_extracted_data_per_frame(PATH):\n",
    "    ''' Takes as an argument the created path for a given segment\n",
    "    '''\n",
    "    # create sub folders to extract the data\n",
    "    CAMERA_IMAGES_DIR = os.path.join(PATH, 'camera_images')\n",
    "    if not os.path.exists(CAMERA_IMAGES_DIR):\n",
    "        os.mkdir(CAMERA_IMAGES_DIR)\n",
    "    RANGE_IMAGES_DIR = os.path.join(PATH, 'range_images')\n",
    "    if not os.path.exists(RANGE_IMAGES_DIR):\n",
    "        os.mkdir(RANGE_IMAGES_DIR)\n",
    "    POINT_CLOUD_DIR = os.path.join(PATH, 'point_clouds')\n",
    "    if not os.path.exists(POINT_CLOUD_DIR):\n",
    "        os.mkdir(POINT_CLOUD_DIR)\n",
    "    LABEL_DIR = os.path.join(PATH, 'labels')\n",
    "    if not os.path.exists(LABEL_DIR):\n",
    "        os.mkdir(LABEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Segment, index: 0, ID: segment-10072140764565668044_4060_000_4080_000\n",
      "Processing segment ID: segment-10072140764565668044_4060_000_4080_000\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# create Dict idx to segment_id\n",
    "# save id and no of frames\n",
    "segments_dict_list = []\n",
    "\n",
    "# Loop over every segment in the dataset\n",
    "for idx in range(len(segments_list)):\n",
    "    segment_dict = {}\n",
    "    # get segment id\n",
    "    segment_id = '_'.join(segments_list[idx].split('_')[:5]) # will get the ID example: 'segment-10072140764565668044_4060_000_4080_000'\n",
    "    segment_dir = os.path.join(data_split_dir, segment_id)\n",
    "    #create folder for the current segment\n",
    "    Path(segment_dir).mkdir(parents=True, exist_ok=True)\n",
    "#     os.mkdir(segment_dir) # create folder for a given semgent\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Reading Segment, index: {}, ID: {}\".format(idx, segment_id))\n",
    "#     add segment id with the respective idx to dict \n",
    "    segment_dict['id'] = segment_id\n",
    "    print(\"Processing segment ID: {}\".format(segment_id))\n",
    "    # get full path of TFRecord file\n",
    "    FILENAME = os.path.join(split_dir, segments_list[idx])\n",
    "    if not os.path.exists(FILENAME):\n",
    "        raise Exception(\"File cannot be found\")\n",
    "    # Read TFRecord\n",
    "    recorded_segment = tf.data.TFRecordDataset(FILENAME, compression_type='')\n",
    "    # Loop over every frame\n",
    "    frame_count = 0\n",
    "    for data in recorded_segment:\n",
    "        # Read the first frame only\n",
    "        frame = open_dataset.Frame()\n",
    "        frame.ParseFromString(bytearray(data.numpy()))\n",
    "        \n",
    "        # extract the camera images, camera projection points and range images\n",
    "        (range_images, \n",
    "        camera_projections, \n",
    "        range_image_top_pose) = frame_utils.parse_range_image_and_camera_projection(frame)\n",
    "\n",
    "        # First return of Lidar data\n",
    "        points, cp_points = frame_utils.convert_range_image_to_point_cloud(\n",
    "            frame,\n",
    "            range_images,\n",
    "            camera_projections,\n",
    "            range_image_top_pose)\n",
    "\n",
    "        # Second return of Lidar data\n",
    "        points_ri2, cp_points_ri2 = frame_utils.convert_range_image_to_point_cloud(\n",
    "            frame,\n",
    "            range_images,\n",
    "            camera_projections,\n",
    "            range_image_top_pose,\n",
    "            ri_index=1)\n",
    "\n",
    "        # concatenate all LIDAR points from the 5 radars.\n",
    "        points_all = np.concatenate(points, axis=0)\n",
    "        points_all_ri2 = np.concatenate(points_ri2, axis=0)\n",
    "        \n",
    "#         print((points_all.dtype))\n",
    "\n",
    "#         # save file on the desk\n",
    "#         pickle_out = open(os.path.join(POINT_CLOUD_DIR, 'point_cloud_{}'.format(segment_id)), 'wb')\n",
    "#         pickle.dump(points_all, pickle_out)\n",
    "#         pickle_out.close()\n",
    "#         pickle_out_ri2 = open(os.path.join(POINT_CLOUD_DIR, 'point_cloud_{}_ri2'.format(segment_id)), 'wb')\n",
    "#         pickle.dump(points_all_ri2, pickle_out_ri2)\n",
    "#         pickle_out_ri2.close()\n",
    "\n",
    "#         # extracting labels and save them\n",
    "#         bboxes = []\n",
    "        bboxes = []\n",
    "        for laser_label in frame.laser_labels:\n",
    "            label = laser_label.type\n",
    "            length = laser_label.box.length\n",
    "            width = laser_label.box.width\n",
    "            height = laser_label.box.height\n",
    "            x, y, z = laser_label.box.center_x, laser_label.box.center_y, laser_label.box.center_z\n",
    "            heading = laser_label.box.heading\n",
    "            box = [label, length, width, height, x, y, z, heading]\n",
    "            bboxes.append(box)\n",
    "#             box = Box([x,y,z], [length, width, height], laser_label.box.heading, label)\n",
    "#             bboxes.append(box)\n",
    "        labels_arr = np.array(bboxes, dtype=np.float32)\n",
    "        file_name = '_'.join([segment_id, str(frame_count)])\n",
    "        np.savez_compressed(os.path.join(segment_dir, '{}.npz'.format(file_name)),pc=points_all, pc_ri2 = points_all_ri2, labels=labels_arr)\n",
    "    \n",
    "        \n",
    "        # save a npz file contains pc_first_return, pc_second_return, bboxes\n",
    "\n",
    "\n",
    "        # save the bounding boxes in their respective directory\n",
    "#         pickle_out = open(os.path.join(LABEL_DIR, 'label_{}'.format(segment_id)), 'wb')\n",
    "#         pickle.dump(bboxes, pickle_out)\n",
    "#         pickle_out.close()\n",
    "        \n",
    "        frame_count += 1\n",
    "        segment_dict['frame_count'] = frame_count\n",
    "    segments_dict_list.append(segment_dict)\n",
    "    break\n",
    "\n",
    "# save idx2segment_id dict on desk\n",
    "pickle_out = open(os.path.join(data_split_dir, 'segments_dict_list'), 'wb')\n",
    "pickle.dump(segments_dict_list, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = []\n",
    "for i in range(5):\n",
    "    arr.append([1,2,3,4,5])\n",
    "final = np.array(arr, dtype=np.float32)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
